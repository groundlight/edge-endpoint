import logging
import os
import shutil
import time
from typing import Dict, Optional

import requests
import yaml
from cachetools import TTLCache, cached
from fastapi import HTTPException, status
from jinja2 import Template

from app.core.file_paths import MODEL_REPOSITORY_PATH
from app.core.speedmon import SpeedMonitor

from .configs import LocalInferenceConfig, RootEdgeConfig

logger = logging.getLogger(__name__)

# Simple TTL cache for is_edge_inference_ready checks to avoid having to re-check every time a request is processed.
# This will be process-specific, so each edge-endpoint worker will have its own cache instance.
ttl_cache = TTLCache(maxsize=128, ttl=2)


@cached(ttl_cache)
def is_edge_inference_ready(inference_client_url: str) -> bool:
    model_ready_url = f"http://{inference_client_url}/health/ready"
    try:
        response = requests.get(model_ready_url)
        return response.status_code == status.HTTP_200_OK
    except requests.exceptions.RequestException as e:
        logger.warning(f"Failed to connect to {model_ready_url}: {e}")
        return False


def submit_image_for_inference(inference_client_url: str, image_bytes: bytes, content_type: str) -> dict:
    inference_url = f"http://{inference_client_url}/infer"
    headers = {"Content-Type": content_type}
    try:
        response = requests.post(inference_url, data=image_bytes, headers=headers)
        if response.status_code != status.HTTP_200_OK:
            logger.error(f"Inference server returned an error: {response.status_code} - {response.text}")
            raise RuntimeError(f"Inference server error: {response.status_code} - {response.text}")
        return response.json()
    except requests.exceptions.RequestException as e:
        logger.error(f"Failed to connect to {inference_url}: {e}")
        raise RuntimeError("Failed to submit image for inference") from e


def parse_inference_response(response: dict) -> dict:
    if "predictions" not in response:
        logger.error(f"Invalid inference response: {response}")
        raise RuntimeError("Invalid inference response")

    # TODO: Clean up and make response parsing more robust.
    # Ideally we would leverage an autogenerated openapi client to handle this.
    #
    # Example response:
    # {
    #     "multi_predictions": None,  # Multiclass / Counting results
    #     "predictions": {"confidences": [0.54], "labels": [0], "probabilities": [0.45], "scores": [-2.94]},  # Binary results
    #     "secondary_predictions": None,  # Text recognition and Obj detection results
    # }
    multi_predictions: dict = response.get("multi_predictions", None)
    predictions: dict = response.get("predictions", None)
    secondary_predictions: dict = response.get("secondary_predictions", None)

    if multi_predictions is not None and predictions is not None:
        raise ValueError("Got result with both multi_predictions and predictions.")
    if multi_predictions is not None:
        # Count or multiclass case
        probabilities: list[float] = multi_predictions["probabilities"][0]
        confidence: float = max(probabilities)
        max_prob_index = max(range(len(probabilities)), key=lambda i: probabilities[i])
        label: int = max_prob_index
    elif predictions is not None:
        # Binary case
        confidence: float = predictions["confidences"][0]
        label: int = predictions["labels"][0]
    else:
        raise ValueError("Got result with no multi_predictions or predictions.")

    rois: list[dict] | None = None
    text: str | None = None
    # Attempt to extract rois / text
    if secondary_predictions is not None:
        roi_predictions: dict[str, list[list[dict]]] | None = secondary_predictions.get("roi_predictions", None)
        text_predictions: list[str] | None = secondary_predictions.get("text_predictions", None)
        if roi_predictions is not None:
            rois = roi_predictions["rois"][0]
            for i, roi in enumerate(rois):
                geometry = roi["geometry"]
                # TODO add validation to calculate x and y automatically
                x = 0.5 * (geometry["left"] + geometry["right"])
                y = 0.5 * (geometry["top"] + geometry["bottom"])
                rois[i]["geometry"]["x"] = x
                rois[i]["geometry"]["y"] = y
        if text_predictions is not None:
            if len(text_predictions) > 1:
                raise ValueError("Got more than one text prediction. This should not happen.")
            text = text_predictions[0]

    output_dict = {"confidence": confidence, "label": label, "text": text, "rois": rois}

    return output_dict


class EdgeInferenceManager:
    INPUT_IMAGE_NAME = "image"
    MODEL_OUTPUTS = ["score", "confidence", "probability", "label"]
    INFERENCE_SERVER_URL = "inference-service:8000"
    MODEL_REPOSITORY = MODEL_REPOSITORY_PATH

    def __init__(
        self,
        inference_configs: Dict[str, LocalInferenceConfig] | None,
        edge_config: RootEdgeConfig,
        verbose: bool = False,
    ) -> None:
        """
        Initializes the edge inference manager.
        Args:
            inference_configs: Dictionary of detector IDs to LocalInferenceConfig objects
            edge_config: RootEdgeConfig object
            verbose: Whether to print verbose logs from the inference server client
        """
        self.verbose = verbose
        self.inference_configs, self.inference_client_urls = {}, {}
        self.speedmon = SpeedMonitor()

        # Last time we escalated to cloud for each detector
        self.last_escalation_times = {detector_id: None for detector_id in edge_config.detectors.keys()}
        # Minimum time between escalations for each detector
        self.min_times_between_escalations = {
            detector_id: detector_config.min_time_between_escalations
            for detector_id, detector_config in edge_config.detectors.items()
        }

        if inference_configs:
            self.inference_configs = inference_configs
            self.inference_client_urls = {
                detector_id: get_edge_inference_service_name(detector_id) + ":8000"
                for detector_id in self.inference_configs.keys()
                if self.detector_configured_for_local_inference(detector_id)
            }

    def update_inference_config(self, detector_id: str, api_token: str) -> None:
        """
        Adds a new detector to the inference config at runtime. This is useful when new
        detectors are added to the database and we want to create an inference deployment for them.
        Args:
            detector_id: ID of the detector on which to run local edge inference
            api_token: API token required to fetch inference models

        """
        if detector_id not in self.inference_configs.keys():
            self.inference_configs[detector_id] = LocalInferenceConfig(enabled=True, api_token=api_token)
            self.inference_client_urls[detector_id] = get_edge_inference_service_name(detector_id) + ":8000"
            logger.info(f"Set up edge inference for {detector_id}")

    def detector_configured_for_local_inference(self, detector_id: str) -> bool:
        """
        Checks if the detector is configured to run local inference.
        Args:
            detector_id: ID of the detector on which to run local edge inference
        Returns:
            True if the detector is configured to run local inference, False otherwise
        """
        if not self.inference_configs:
            return False

        return detector_id in self.inference_configs.keys() and self.inference_configs[detector_id].enabled

    def inference_is_available(self, detector_id: str) -> bool:
        """
        Queries the inference server to see if everything is ready to perform inference.
        Args:
            detector_id: ID of the detector on which to run local edge inference
        Returns:
            True if edge inference for the specified detector is available, False otherwise
        """
        try:
            inference_client_url = self.inference_client_urls[detector_id]
        except KeyError:
            logger.debug(f"Failed to look up inference client for {detector_id}")
            return False

        if not is_edge_inference_ready(inference_client_url):
            logger.debug("Edge inference server is not ready")
            return False
        return True

    def run_inference(self, detector_id: str, image_bytes: bytes, content_type: str) -> dict:
        """
        Submit an image to the inference server, route to a specific model, and return the results.
        Args:
            detector_id: ID of the detector on which to run local edge inference
            image_bytes: The serialized image to submit for inference
            content_type: The content type of the image
        Returns:
            Dictionary of inference results with keys:
                - "score": float
                - "confidence": float
                - "probability": float
                - "label": str
        """
        logger.info(f"Submitting image to edge inference service. {detector_id=}")
        start_time = time.perf_counter()

        inference_client_url = self.inference_client_urls[detector_id]
        response = submit_image_for_inference(inference_client_url, image_bytes, content_type)
        output_dict = parse_inference_response(response)

        elapsed_ms = (time.perf_counter() - start_time) * 1000
        self.speedmon.update(detector_id, elapsed_ms)
        fps = self.speedmon.average_fps(detector_id)

        logger.debug(f"Inference server response for request {detector_id=}: {output_dict}.")
        logger.info(f"Recent-average FPS for {detector_id=}: {fps:.2f}")
        return output_dict

    def update_model(self, detector_id: str) -> bool:
        """
        Request a new model from Groundlight. If there is a new model available, download it and
        write it to the model repository as a new version.

        Returns True if a new model was downloaded and saved, False otherwise.
        """
        logger.info(f"Checking if there is a new model available for {detector_id}")

        api_token = (
            self.inference_configs[detector_id].api_token
            if self.detector_configured_for_local_inference(detector_id)
            else None
        )

        # fallback to env var if we dont have a token in the config
        api_token = api_token or os.environ.get("GROUNDLIGHT_API_TOKEN", None)

        model_urls = fetch_model_urls(detector_id, api_token=api_token)
        cloud_binary_ksuid = model_urls.get("model_binary_id", None)
        if cloud_binary_ksuid is None:
            logger.warning(f"No model binary ksuid returned for {detector_id}")

        model_dir = os.path.join(self.MODEL_REPOSITORY, detector_id)
        edge_binary_ksuid = get_current_model_ksuid(model_dir)
        if edge_binary_ksuid and cloud_binary_ksuid is not None and cloud_binary_ksuid <= edge_binary_ksuid:
            logger.info(f"No new model available for {detector_id}")
            return False

        logger.info(f"New model binary available ({cloud_binary_ksuid}), attemping to update model for {detector_id}")

        pipeline_config = model_urls["pipeline_config"]
        predictor_metadata = model_urls["predictor_metadata"]
        model_buffer = get_object_using_presigned_url(model_urls["model_binary_url"])
        save_model_to_repository(
            detector_id,
            model_buffer,
            pipeline_config,
            predictor_metadata,
            binary_ksuid=cloud_binary_ksuid,
            repository_root=self.MODEL_REPOSITORY,
        )
        return True

    def escalation_cooldown_complete(self, detector_id: str) -> bool:
        """
        Check if the time since the last escalation is long enough ago that we should escalate again.
        The minimum time between escalations for a detector is set by the `min_time_between_escalations` field in the
        detector's config. If the field is not set, we use a default of 2 seconds.

        Args:
            detector_id: ID of the detector to check
        Returns:
            True if there hasn't been an escalation on this detector in the last `min_time_between_escalations` seconds, False otherwise.
        """
        min_time_between_escalations = self.min_times_between_escalations.get(detector_id, 2)

        if (
            self.last_escalation_times[detector_id] is None
            or (time.time() - self.last_escalation_times[detector_id]) > min_time_between_escalations
        ):
            self.last_escalation_times[detector_id] = time.time()
            return True

        return False


def fetch_model_urls(detector_id: str, api_token: Optional[str] = None) -> dict[str, str]:
    if not api_token:
        raise ValueError(f"No API token provided for {detector_id=}")

    logger.debug(f"Fetching model URLs for {detector_id}")

    url = f"https://api.groundlight.ai/edge-api/v1/fetch-model-urls/{detector_id}/"
    headers = {"x-api-token": api_token}
    response = requests.get(url, headers=headers, timeout=10)
    logger.debug(f"fetch-model-urls response = {response}")

    if response.status_code == status.HTTP_200_OK:
        return response.json()
    else:
        raise HTTPException(status_code=response.status_code, detail=f"Failed to fetch model URLs for {detector_id=}.")


def get_object_using_presigned_url(presigned_url: str) -> bytes:
    response = requests.get(presigned_url, timeout=10)
    if response.status_code == status.HTTP_200_OK:
        return response.content
    else:
        raise HTTPException(status_code=response.status_code, detail=f"Failed to retrieve data from {presigned_url}.")


def save_model_to_repository(
    detector_id: str,
    model_buffer: bytes,
    pipeline_config: str,
    predictor_metadata: str,
    binary_ksuid: Optional[str],
    repository_root: str,
) -> tuple[Optional[int], int]:
    """
    Make new version-directory for the model and save the new version of the model and pipeline config to it.
    Model repository directory structure:
    ```
    <model-repository-path>/
        <model-name>/
            <version>/
                <model-definition-files (e.g. model.buf, pipeline_config.yaml, etc)>
    ```
    """
    model_dir = os.path.join(repository_root, detector_id)
    os.makedirs(model_dir, exist_ok=True)

    old_model_version = get_current_model_version(model_dir)
    new_model_version = 1 if old_model_version is None else old_model_version + 1

    model_version_dir = os.path.join(model_dir, str(new_model_version))
    os.makedirs(model_version_dir, exist_ok=True)

    with open(os.path.join(model_version_dir, "model.buf"), "wb") as f:
        f.write(model_buffer)
    with open(os.path.join(model_version_dir, "pipeline_config.yaml"), "w") as f:
        yaml.dump(yaml.safe_load(pipeline_config), f)
    with open(os.path.join(model_version_dir, "predictor_metadata.json"), "w") as f:
        f.write(predictor_metadata)
    if binary_ksuid:
        with open(os.path.join(model_version_dir, "model_id.txt"), "w") as f:
            f.write(binary_ksuid)

    logger.info(f"Wrote new model version {new_model_version} for {detector_id} with {binary_ksuid=}")
    return old_model_version, new_model_version


def get_current_model_version(model_dir: str) -> Optional[int]:
    """Edge inference server model_repositories contain model versions in subdirectories. These subdirectories
    are named with integers. This function returns the highest integer in the model repository directory.
    """
    logger.debug(f"Checking for current model version in {model_dir}")
    model_versions = get_all_model_versions(model_dir)
    return max(model_versions) if len(model_versions) > 0 else None


def get_all_model_versions(model_dir: str) -> list:
    """Edge inference server model_repositories contain model versions in subdirectories.
    Return all such version numbers.
    """
    if not os.path.exists(model_dir):
        return []
    model_versions = [int(d) for d in os.listdir(model_dir) if os.path.isdir(os.path.join(model_dir, d))]
    return model_versions


def get_current_model_ksuid(model_dir: str) -> Optional[str]:
    """Read the model_id.txt file in the current model version directory,
    which contains the KSUID of the model binary.
    """
    v = get_current_model_version(model_dir)
    if v is None:
        logger.info(f"No current model version found in {model_dir}")
        return None
    id_file = os.path.join(model_dir, str(v), "model_id.txt")
    if os.path.exists(id_file):
        with open(id_file, "r") as f:
            return f.read()
    else:
        logger.warning(f"No existing model_id.txt file found in {os.path.join(model_dir, str(v))}")
        return None


def create_file_from_template(template_values: dict, destination: str, template: str) -> None:
    """
    This is a helper function to create a file from a Jinja2 template. In your template file,
    place template values in {{ template_value }} blocks. Then pass in a dictionary mapping template
    keys to values. The template will be filled with the values and written to the destination file.

    See https://jinja.palletsprojects.com/en/3.1.x/templates/ for more information on Jinja2 templates.
    """
    # Step 1: Read the template file
    with open(template, "r") as template_file:
        template_content = template_file.read()

    # Step 2: Substitute placeholders with actual values
    template = Template(template_content)
    filled_content = template.render(**template_values)

    # Step 3: Write the filled content to a new file
    os.makedirs(os.path.dirname(destination), exist_ok=True)
    with open(destination, "w") as output_file:
        output_file.write(filled_content)


def delete_old_model_versions(detector_id: str, repository_root: str, num_to_keep: int = 2) -> None:
    """Recursively delete all but the latest model versions"""
    model_dir = os.path.join(repository_root, detector_id)
    model_versions = get_all_model_versions(model_dir)
    model_versions = sorted(model_versions)
    if len(model_versions) < num_to_keep:
        return
    versions_to_delete = model_versions[:-num_to_keep]  # all except the last num_to_keep
    logger.info(f"Deleting {len(versions_to_delete)} old model version(s) for {detector_id}")
    for v in versions_to_delete:
        delete_model_version(detector_id, v, repository_root)


def delete_model_version(detector_id: str, model_version: int, repository_root: str) -> None:
    """Recursively delete directory detector_id/model_version"""
    model_version_dir = os.path.join(repository_root, detector_id, str(model_version))
    logger.info(f"Deleting model version {model_version} for {detector_id}")
    if os.path.exists(model_version_dir):
        shutil.rmtree(model_version_dir)


def get_edge_inference_service_name(detector_id: str) -> str:
    """
    Kubernetes service/deployment names have a strict naming convention.
    They have to be alphanumeric, lower cased, and can only contain dashes.
    We just use `inferencemodel-<detector_id>` as the deployment name and
    `inference-service-<detector_id>` as the service name.
    """
    return f"inference-service-{detector_id.replace('_', '-').lower()}"


def get_edge_inference_deployment_name(detector_id: str) -> str:
    return f"inferencemodel-{detector_id.replace('_', '-').lower()}"

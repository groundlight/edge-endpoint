import logging
import os
import shutil
import time
from typing import Optional

import requests
import yaml
from cachetools import TTLCache, cached
from fastapi import HTTPException, status
from jinja2 import Template

from app.core.configs import EdgeInferenceConfig
from app.core.file_paths import MODEL_REPOSITORY_PATH
from app.core.speedmon import SpeedMonitor
from app.core.utils import ModelInfoBase, ModelInfoWithBinary, parse_model_info

logger = logging.getLogger(__name__)

# Simple TTL cache for is_edge_inference_ready checks to avoid having to re-check every time a request is processed.
# This will be process-specific, so each edge-endpoint worker will have its own cache instance.
ttl_cache = TTLCache(maxsize=128, ttl=2)


@cached(ttl_cache)
def is_edge_inference_ready(inference_client_url: str) -> bool:
    model_ready_url = f"http://{inference_client_url}/health/ready"
    try:
        response = requests.get(model_ready_url)
        return response.status_code == status.HTTP_200_OK
    except requests.exceptions.RequestException as e:
        logger.warning(f"Failed to connect to {model_ready_url}: {e}")
        return False


def submit_image_for_inference(inference_client_url: str, image_bytes: bytes, content_type: str) -> dict:
    inference_url = f"http://{inference_client_url}/infer"
    headers = {"Content-Type": content_type}
    try:
        logger.debug(f"Submitting image for inference to {inference_url}")
        response = requests.post(inference_url, data=image_bytes, headers=headers)
        if response.status_code != status.HTTP_200_OK:
            logger.error(f"Inference server returned an error: {response.status_code} - {response.text}")
            raise RuntimeError(f"Inference server error: {response.status_code} - {response.text}")
        return response.json()
    except requests.exceptions.RequestException as e:
        logger.error(f"Failed to connect to {inference_url}: {e}")
        raise RuntimeError("Failed to submit image for inference") from e


def parse_inference_response(response: dict) -> dict:
    if "predictions" not in response:
        logger.error(f"Invalid inference response: {response}")
        raise RuntimeError("Invalid inference response")

    # TODO: Clean up and make response parsing more robust.
    # Ideally we would leverage an autogenerated openapi client to handle this.
    #
    # Example response:
    # {
    #     "multi_predictions": None,  # Multiclass / Counting results
    #     "predictions": {"confidences": [0.54], "labels": [0], "probabilities": [0.45], "scores": [-2.94]},  # Binary results
    #     "secondary_predictions": None,  # Text recognition and Obj detection results
    # }
    multi_predictions: dict = response.get("multi_predictions", None)
    predictions: dict = response.get("predictions", None)
    secondary_predictions: dict = response.get("secondary_predictions", None)

    if multi_predictions is not None and predictions is not None:
        raise ValueError("Got result with both multi_predictions and predictions.")
    if multi_predictions is not None:
        # Count or multiclass case
        probabilities: list[float] = multi_predictions["probabilities"][0]
        confidence: float = max(probabilities)
        max_prob_index = max(range(len(probabilities)), key=lambda i: probabilities[i])
        label: int = max_prob_index
    elif predictions is not None:
        # Binary case
        confidence: float = predictions["confidences"][0]
        label: int = predictions["labels"][0]
    else:
        raise ValueError("Got result with no multi_predictions or predictions.")

    rois: list[dict] | None = None
    text: str | None = None
    # Attempt to extract rois / text
    if secondary_predictions is not None:
        roi_predictions: dict[str, list[list[dict]]] | None = secondary_predictions.get("roi_predictions", None)
        text_predictions: list[str] | None = secondary_predictions.get("text_predictions", None)
        if roi_predictions is not None:
            rois = roi_predictions["rois"][0]
            for i, roi in enumerate(rois):
                geometry = roi["geometry"]
                # TODO add validation to calculate x and y automatically
                x = 0.5 * (geometry["left"] + geometry["right"])
                y = 0.5 * (geometry["top"] + geometry["bottom"])
                rois[i]["geometry"]["x"] = x
                rois[i]["geometry"]["y"] = y
        if text_predictions is not None:
            if len(text_predictions) > 1:
                raise ValueError("Got more than one text prediction. This should not happen.")
            text = text_predictions[0]

    output_dict = {"confidence": confidence, "label": label, "text": text, "rois": rois}

    return output_dict


class EdgeInferenceManager:
    INPUT_IMAGE_NAME = "image"
    MODEL_OUTPUTS = ["score", "confidence", "probability", "label"]
    INFERENCE_SERVER_URL = "inference-service:8000"
    MODEL_REPOSITORY = MODEL_REPOSITORY_PATH

    def __init__(
        self,
        detector_inference_configs: dict[str, EdgeInferenceConfig] | None,
        verbose: bool = False,
    ) -> None:
        """
        Initializes the edge inference manager.
        Args:
            detector_inference_configs: Dictionary of detector IDs to EdgeInferenceConfig objects
            edge_config: RootEdgeConfig object
            verbose: Whether to print verbose logs from the inference server client
        """
        self.verbose = verbose
        self.detector_inference_configs, self.inference_client_urls, self.oodd_inference_client_urls = {}, {}, {}
        self.speedmon = SpeedMonitor()

        if detector_inference_configs:
            self.detector_inference_configs = detector_inference_configs
            self.inference_client_urls = {
                detector_id: get_edge_inference_service_name(detector_id) + ":8000"
                for detector_id in self.detector_inference_configs.keys()
                if self.detector_configured_for_edge_inference(detector_id)
            }
            self.oodd_inference_client_urls = {
                detector_id: get_edge_inference_service_name(detector_id, is_oodd=True) + ":8000"
                for detector_id in self.detector_inference_configs.keys()
                if self.detector_configured_for_edge_inference(detector_id)
            }

        # Last time we escalated to cloud for each detector
        self.last_escalation_times = {detector_id: None for detector_id in self.detector_inference_configs.keys()}
        # Minimum time between escalations for each detector
        self.min_times_between_escalations = {
            detector_id: detector_inference_config.min_time_between_escalations
            for detector_id, detector_inference_config in self.detector_inference_configs.items()
        }

    def update_inference_config(self, detector_id: str, api_token: str) -> None:
        """
        Adds a new detector to the inference config at runtime. This is useful when new
        detectors are added to the database and we want to create an inference deployment for them.
        Args:
            detector_id: ID of the detector on which to run local edge inference
            api_token: API token required to fetch inference models

        """
        if detector_id not in self.detector_inference_configs.keys():
            self.detector_inference_configs[detector_id] = EdgeInferenceConfig(enabled=True, api_token=api_token)
            self.inference_client_urls[detector_id] = get_edge_inference_service_name(detector_id) + ":8000"
            self.oodd_inference_client_urls[detector_id] = (
                get_edge_inference_service_name(detector_id, is_oodd=True) + ":8000"
            )
            logger.info(f"Set up edge inference for {detector_id}")

    def detector_configured_for_edge_inference(self, detector_id: str) -> bool:
        """
        Checks if the detector is configured to run local inference.
        Args:
            detector_id: ID of the detector on which to run local edge inference
        Returns:
            True if the detector is configured to run local inference, False otherwise
        """
        if not self.detector_inference_configs:
            return False

        return (
            detector_id in self.detector_inference_configs.keys()
            and self.detector_inference_configs[detector_id].enabled
        )

    def inference_is_available(self, detector_id: str) -> bool:
        """
        Queries the inference server to see if everything is ready to perform inference.
        Args:
            detector_id: ID of the detector on which to run local edge inference
        Returns:
            True if edge inference for the specified detector is available, False otherwise
        """
        try:
            inference_client_url = self.inference_client_urls[detector_id]
            oodd_inference_client_url = self.oodd_inference_client_urls[detector_id]
        except KeyError:
            logger.info(f"Failed to look up inference clients for {detector_id}")
            return False

        if not is_edge_inference_ready(inference_client_url):
            logger.debug("Edge inference server is not ready")
            return False
        if not is_edge_inference_ready(oodd_inference_client_url):
            logger.debug("OODD inference server is not ready")
            return False
        return True

    def run_inference(self, detector_id: str, image_bytes: bytes, content_type: str) -> dict:
        """
        Submit an image to the inference server, route to a specific model, and return the results.
        Args:
            detector_id: ID of the detector on which to run local edge inference
            image_bytes: The serialized image to submit for inference
            content_type: The content type of the image
        Returns:
            Dictionary of inference results with keys:
                - "score": float
                - "confidence": float
                - "probability": float
                - "label": str
        """
        logger.info(f"Submitting image to edge inference service. {detector_id=}")
        start_time = time.perf_counter()

        inference_client_url = self.inference_client_urls[detector_id]
        oodd_inference_client_url = self.oodd_inference_client_urls[detector_id]

        response = submit_image_for_inference(inference_client_url, image_bytes, content_type)
        oodd_response = submit_image_for_inference(oodd_inference_client_url, image_bytes, content_type)

        output_dict = parse_inference_response(response)
        oodd_output_dict = parse_inference_response(oodd_response)

        logger.info(f"Response from edge model inference: {output_dict}")
        logger.info(f"Response from OODD model inference: {oodd_output_dict}")

        elapsed_ms = (time.perf_counter() - start_time) * 1000
        self.speedmon.update(detector_id, elapsed_ms)
        fps = self.speedmon.average_fps(detector_id)

        logger.debug(f"Inference server response for request {detector_id=}: {output_dict}.")
        logger.info(f"Recent-average FPS for {detector_id=}: {fps:.2f}")
        return output_dict

    def update_models_if_available(self, detector_id: str) -> bool:
        """
        Request a new model from Groundlight. If there is a new model available, download it and
        write it to the model repository as a new version.

        Returns True if a new model was downloaded and saved, False otherwise.
        """
        logger.info(f"Checking if there is a new model available for {detector_id}")

        api_token = (
            self.detector_inference_configs[detector_id].api_token
            if self.detector_configured_for_edge_inference(detector_id)
            else None
        )

        # fallback to env var if we dont have a token in the config
        api_token = api_token or os.environ.get("GROUNDLIGHT_API_TOKEN", None)

        edge_model_dir = os.path.join(self.MODEL_REPOSITORY, detector_id)
        oodd_model_dir = os.path.join(self.MODEL_REPOSITORY, detector_id + "_oodd")

        edge_v = get_current_model_version(edge_model_dir)
        oodd_v = get_current_model_version(oodd_model_dir)

        edge_model_info, oodd_model_info = fetch_model_info(detector_id, api_token=api_token)

        new_edge_model_available = should_update(edge_model_info, edge_model_dir, edge_v)
        new_oodd_model_available = should_update(oodd_model_info, oodd_model_dir, oodd_v)

        if not new_edge_model_available and not new_oodd_model_available:
            logger.info(f"No new models available for {detector_id}")
            return False

        self.update_model(edge_model_info, detector_id)
        self.update_model(oodd_model_info, detector_id, is_oodd=True)

        return True

    def update_model(self, model_info: ModelInfoBase, detector_id: str, is_oodd: bool = False) -> None:
        if isinstance(model_info, ModelInfoWithBinary):
            logger.info(
                f"New model binary available ({model_info.model_binary_id}), attemping to update model "
                f"for {detector_id}"
            )
            model_buffer = get_object_using_presigned_url(model_info.model_binary_url)
        else:
            logger.info(f"Got a pipeline config but no model binary, attempting to update model for {detector_id}")
            model_buffer = None

        if is_oodd:
            model_dir = os.path.join(self.MODEL_REPOSITORY, detector_id + "_oodd")
        else:
            model_dir = os.path.join(self.MODEL_REPOSITORY, detector_id)

        save_model_to_repository(
            model_buffer,
            model_info,
            model_dir,
        )


    def escalation_cooldown_complete(self, detector_id: str) -> bool:
        """
        Check if the time since the last escalation is long enough ago that we should escalate again.
        The minimum time between escalations for a detector is set by the `min_time_between_escalations` field in the
        detector's config. If the field is not set, we use a default of 2 seconds.

        Args:
            detector_id: ID of the detector to check
        Returns:
            True if there hasn't been an escalation on this detector in the last `min_time_between_escalations` seconds,
              False otherwise.
        """
        min_time_between_escalations = self.min_times_between_escalations.get(detector_id, 2)
        last_escalation_time = self.last_escalation_times[detector_id]

        if last_escalation_time is None or (time.time() - last_escalation_time) > min_time_between_escalations:
            self.last_escalation_times[detector_id] = time.time()
            return True

        return False


def fetch_model_info(detector_id: str, api_token: Optional[str] = None) -> tuple[ModelInfoBase, ModelInfoBase]:
    """
    Fetch the model info for a detector from the Groundlight API at the fetch-model-urls endpoint.
    Gets information about both the primary edge model and the OODD model.
    Args:
        detector_id: The ID of the detector to fetch the model info for
        api_token: The API token to use to fetch the model info
    Returns:
        A tuple of two ModelInfoBase objects, one for the edge model and one for the OODD model
    """
    if not api_token:
        raise ValueError(f"No API token provided for {detector_id=}")

    logger.info(f"Fetching model info for {detector_id}")

    url = f"https://api.groundlight.ai/edge-api/v1/fetch-model-urls/{detector_id}/"
    headers = {"x-api-token": api_token}
    response = requests.get(url, headers=headers, timeout=10)
    logger.debug(f"Received response from fetch-model-urls: {response.json()}")

    if response.status_code == status.HTTP_200_OK:
        return parse_model_info(response.json())
    else:
        response_json = response.json()
        exception_string = f"Failed to fetch model info for {detector_id=}."
        if "detail" in response_json:  # Include additional detail on the error if available
            exception_string = exception_string + f" Received error: {response_json['detail']}"

        raise HTTPException(status_code=response.status_code, detail=exception_string)


def get_object_using_presigned_url(presigned_url: str) -> bytes:
    response = requests.get(presigned_url, timeout=10)
    if response.status_code == status.HTTP_200_OK:
        return response.content
    else:
        raise HTTPException(status_code=response.status_code, detail=f"Failed to retrieve data from {presigned_url}.")


def save_model_to_repository(
    model_buffer: Optional[bytes],
    model_info: ModelInfoBase,
    model_dir: str,
) -> tuple[Optional[int], int]:
    """
    Make new version-directory for the model and save the new version of the model and pipeline config to it.
    Model repository directory structure:
    ```
    <model-repository-path>/
        <model-name>/
            <version>/
                <model-definition-files (e.g. model.buf, pipeline_config.yaml, etc)>
    ```
    """
    os.makedirs(model_dir, exist_ok=True)

    old_model_version = get_current_model_version(model_dir)
    new_model_version = 1 if old_model_version is None else old_model_version + 1

    model_version_dir = os.path.join(model_dir, str(new_model_version))
    os.makedirs(model_version_dir, exist_ok=True)

    if model_buffer:
        with open(os.path.join(model_version_dir, "model.buf"), "wb") as f:
            f.write(model_buffer)
    with open(os.path.join(model_version_dir, "pipeline_config.yaml"), "w") as f:
        yaml.dump(yaml.safe_load(model_info.pipeline_config), f)
    with open(os.path.join(model_version_dir, "predictor_metadata.json"), "w") as f:
        f.write(model_info.predictor_metadata)
    if isinstance(model_info, ModelInfoWithBinary):
        with open(os.path.join(model_version_dir, "model_id.txt"), "w") as f:
            f.write(model_info.model_binary_id)

    logger.info(
        f"Wrote new model version {new_model_version} to {model_dir}"
        + (f" with model binary id {model_info.model_binary_id}" if isinstance(model_info, ModelInfoWithBinary) else "")
    )

    return old_model_version, new_model_version


def should_update(model_info: Optional[ModelInfoBase], model_dir: str, version: int) -> bool:
    """Determines if the model needs to be updated based on the received and current model info."""
    if isinstance(model_info, ModelInfoWithBinary):
        edge_binary_ksuid = get_current_model_ksuid(model_dir, version)
        if edge_binary_ksuid and model_info.model_binary_id == edge_binary_ksuid:
            logger.info(f"The edge binary is the same as the cloud binary, so we don't need to update the model.")
            return False
    elif model_info is not None:
        current_pipeline_config = get_current_pipeline_config(model_dir, version)
        if current_pipeline_config and current_pipeline_config == yaml.safe_load(model_info.pipeline_config):
            logger.info(f"There is no saved binary and the current pipeline_config is the same as the received pipeline_config, so we don't need to update the model.")
            return False
    else:
        logger.info(f"No current model version found in {model_dir}.")

    return True


def get_current_model_version(model_dir: str) -> Optional[int]:
    """Edge inference server model_repositories contain model versions in subdirectories. These subdirectories
    are named with integers. This function returns the highest integer in the model repository directory.
    """
    logger.debug(f"Checking for current model version in {model_dir}")
    model_versions = get_all_model_versions(model_dir)
    return max(model_versions) if len(model_versions) > 0 else None


def get_all_model_versions(model_dir: str) -> list:
    """Edge inference server model_repositories contain model versions in subdirectories.
    Return all such version numbers.
    """
    if not os.path.exists(model_dir):
        return []
    model_versions = [int(d) for d in os.listdir(model_dir) if os.path.isdir(os.path.join(model_dir, d))]
    return model_versions


def get_current_model_ksuid(model_dir: str, model_version: int) -> Optional[str]:
    """Read the model_id.txt file in the current model version directory,
    which contains the KSUID of the model binary (if available).
    """
    id_file = os.path.join(model_dir, str(model_version), "model_id.txt")
    if os.path.exists(id_file):
        with open(id_file, "r") as f:
            return f.read()
    else:
        logger.debug(f"No existing model_id.txt file found in {os.path.join(model_dir, str(model_version))}")
        return None


def get_current_pipeline_config(model_dir: str, model_version: int) -> dict | None:
    """Read the pipeline_config.yaml file in the current model version directory."""
    config_file = os.path.join(model_dir, str(model_version), "pipeline_config.yaml")
    if os.path.exists(config_file):
        with open(config_file, "r") as f:
            return yaml.safe_load(f)
    else:
        logger.warning(f"No existing pipeline_config.yaml file found in {os.path.join(model_dir, str(model_version))}")
        return None


def create_file_from_template(template_values: dict, destination: str, template: str) -> None:
    """
    This is a helper function to create a file from a Jinja2 template. In your template file,
    place template values in {{ template_value }} blocks. Then pass in a dictionary mapping template
    keys to values. The template will be filled with the values and written to the destination file.

    See https://jinja.palletsprojects.com/en/3.1.x/templates/ for more information on Jinja2 templates.
    """
    # Step 1: Read the template file
    with open(template, "r") as template_file:
        template_content = template_file.read()

    # Step 2: Substitute placeholders with actual values
    template = Template(template_content)
    filled_content = template.render(**template_values)

    # Step 3: Write the filled content to a new file
    os.makedirs(os.path.dirname(destination), exist_ok=True)
    with open(destination, "w") as output_file:
        output_file.write(filled_content)


def delete_old_model_versions(detector_id: str, repository_root: str, num_to_keep: int = 2) -> None:
    """Recursively delete all but the latest model versions"""
    model_dir = os.path.join(repository_root, detector_id)
    model_versions = get_all_model_versions(model_dir)
    model_versions = sorted(model_versions)
    if len(model_versions) < num_to_keep:
        return
    versions_to_delete = model_versions[:-num_to_keep]  # all except the last num_to_keep
    logger.info(f"Deleting {len(versions_to_delete)} old model version(s) for {detector_id}")
    for v in versions_to_delete:
        delete_model_version(detector_id, v, repository_root)


def delete_model_version(detector_id: str, model_version: int, repository_root: str) -> None:
    """Recursively delete directory detector_id/model_version"""
    model_version_dir = os.path.join(repository_root, detector_id, str(model_version))
    logger.info(f"Deleting model version {model_version} for {detector_id}")
    if os.path.exists(model_version_dir):
        shutil.rmtree(model_version_dir)


def get_edge_inference_service_name(detector_id: str, is_oodd: bool = False) -> str:
    """
    Kubernetes service/deployment names have a strict naming convention.
    They have to be alphanumeric, lower cased, and can only contain dashes.
    We just use `inferencemodel-<detector_id>` as the deployment name and
    `inference-service-<detector_id>` as the service name. If the deployment/service
    is for an OODD model, we append "-oodd" to the name.
    """
    service_name = f"inference-service-{detector_id.replace('_', '-').lower()}"
    if is_oodd:
        service_name += "-oodd"
    return service_name


def get_edge_inference_deployment_name(detector_id: str, is_oodd: bool = False) -> str:
    deployment_name = f"inferencemodel-{detector_id.replace('_', '-').lower()}"
    if is_oodd:
        deployment_name += "-oodd"
    return deployment_name

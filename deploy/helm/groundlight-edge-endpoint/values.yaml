# Default values for groundlight-edge-endpoint.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# The Kubernetees namespace where the Edge Endpoint will be deployed
# We recommend that you don't change this value unless you're doing something unusual
# like running more that one instance of the Edge Endpoint in the same cluster
namespace: "edge"

# The tag to use for the images. The value of `imageTag` is used for both the edge-endpoint and
# inference images, unless you add a specific tag for one or the other image
imageTag: "release"
edgeEndpointTag: ""
inferenceTag: ""

# Whether to use the minimal image for the inference server. Currently, the minimal image only
# supports binary and multiclass detectors, so be sure to keep this set to false if you're using
# an object detection or counting model. 
useMinimalImage: false

# The image pull policy for the containers.
# The default value is "Always" which means that Kubernetes will always check to see if there's 
# a new version of the image with the requested tag available when starting containers.
imagePullPolicy: "Always"

# The port that the Edge Endpoint will listen on on the host. 
# Within the cluster, edge-endpoint always listens on port 30101 and can be addressed by 
# other services in the cluster on that port using
# http://edge-endpoint-service.edge.svc.cluster.local:30101/ (or substitute the appropriate
# namespace if you've overriden the default value).
edgeEndpointPort: 30101

# This is used as the base of the name for the PersistentVolume. The full name of the volume is 
# created by appending the namespace to this value. Generally, this should not be overridden.
# The PersistentVolume is used to store the model files and other data that the Edge Endpoint wants
# to persist between restarts. It is mapped to `/opt/groundlight/edge` on the host.
persistentVolumeNameBase: "edge-endpoint-pv"

# Edge Endpoint will default to using the GPU for inference. If you want to use the CPU instead,
# set this value to "cpu".
inferenceFlavor: "gpu"

# The user must provide the Groundlight API token as input or the deployment will fail
groundlightApiToken: ""

# For escalations and audits and calls that aren't handled by the Edge Endpoint, we forward
# to the Groundlight service in the cloud. If you're testing against another version
# of the Groundlight service (e.g., your own dev environment or the integ environment),
# you can override this value.
upstreamEndpoint: "https://api.groundlight.ai"

# Currently, all Groundlight services are deployed in the us-west-2 region
awsRegion: "us-west-2"

ecrRegistry: "767397850842.dkr.ecr.us-west-2.amazonaws.com"

# This sets the log level for all the containers, both edge endpoint and inference.
logLevel: "INFO"

# These values override the automated settings in _helpers.tpl to keep this short and sweet
# Don't override these
nameOverride: "edge-endpoint"
fullnameOverride: "edge-endpoint"

# Set some sensible limits on memory usage to avoid system crashing
k3sConfig:
  enabled: true
  evictionHardPercent: "15"
  evictionSoftPercent: "25" 
  evictionHardMinGB: "4"
  evictionSoftMinGB: "8"
  evictionGracePeriod: "10s"

serviceAccount:
  create: true
  name: ""

# Splunk Configuration
splunk:
  enabled: false  # Set to true to enable Splunk deployment
  image:
    repository: splunk/splunk
    tag: "9.3.5"
  password: "admin123"
  hecToken: "abcd1234-5678-90ef-ghij-klmnopqrstuv"
  service:
    type: NodePort
    webNodePort: 30080    # Splunk Web UI
    hecNodePort: 30088    # HTTP Event Collector
    managementNodePort: 30089  # Management port
  persistence:
    dataSize: "10Gi"
    etcSize: "2Gi"
    storageClass: ""  # Use default storage class
  resources:
    limits:
      cpu: 2000m
      memory: 4Gi
    requests:
      cpu: 500m
      memory: 2Gi
apiVersion: v1
kind: Service
metadata:
  name: edge-endpoint-service
spec:
  selector:
    app: edge-logic-server
  ports:
  - protocol: TCP
    # Service port for NGINX
    port: 6717
  type: NodePort
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: edge-endpoint
  labels:
    app: edge-endpoint
spec:
  replicas: 1
  selector:
    matchLabels:
      app: edge-logic-server
  template:
    metadata:
      labels:
        app: edge-logic-server
    spec:
      # This service account is used by the edge-endpoint pod to access the Kubernetes API
      # from within the pod. See deploy/k3s/service_account.yaml for more details
      serviceAccountName: edge-endpoint-service-account
      containers:
      - name: edge-endpoint
        # Pull the image from ECR
        # image: 723181461334.dkr.ecr.us-west-2.amazonaws.com/edge-endpoint:main-39ba8639f-dirty-914eb756cb2740c
        image: 723181461334.dkr.ecr.us-west-2.amazonaws.com/edge-endpoint:state-management-via-k3s-7bd94141a-dirty-461ecb38a645c37
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 6717
        env:
        - name: LOG_LEVEL
          value: "DEBUG"
        - name: GROUNDLIGHT_API_TOKEN
          valueFrom:
            secretKeyRef:
              name: groundlight-secrets
              key: api-token
        volumeMounts:
        - name: edge-config-volume
          # This is a path inside the container not the host
          mountPath: /etc/groundlight
      imagePullSecrets:
      - name: registry-credentials

      volumes:
      - name: edge-config-volume
        configMap:
          name: edge-config
# ---
# apiVersion: v1
# kind: Service
# metadata:
#   name: inference-service
# spec:
#   selector:
#     app: inference-server
#   ports:
#     - protocol: TCP
#       port: 8000
#       targetPort: 8000
#   type: ClusterIP
# ---
# apiVersion: apps/v1
# kind: Deployment
# metadata:
#   name: inference-server
#   labels:
#     app: inference-server
# spec:
#   replicas: 1
#   selector:
#     matchLabels:
#       app: inference-server
#   template:
#     metadata:
#       labels:
#         app: inference-server
#     spec:
#       containers:
#       - name: inference-server
#         image: 723181461334.dkr.ecr.us-west-2.amazonaws.com/gl-tritonserver:main-749c0cca3-dirty-0325a8beabe0d7b
#         imagePullPolicy: IfNotPresent
#         # Tritonserver will look for models in /mnt/models and initialize them on startup
#         # TODO: We only want to shut down an old instance when this tritonsever gives the "READY" signal for all models
#         command:
#           ["tritonserver", "--model-repository=/mnt/models"]
#         volumeMounts:
#         - name: model-repo
#           mountPath: /mnt/models
#         - name: dshm
#           mountPath: /dev/shm
#         ports:
#         - containerPort: 8000  # Default tritonserver HTTP port
#       imagePullSecrets:
#       - name: registry-credentials
#       volumes:
#       - name: model-repo
#         hostPath:
#           # A hostPath volume mounts a file or directory from the host node's filesystem into your Pod
#           # This simiplifies things a lot, but only really works for single-node clusters.
#           # TODO: check out k3s local path provisioner: https://docs.k3s.io/storage#setting-up-the-local-storage-provider
#           path: /home/ubuntu/ptdev/zuuul2/predictors/serving/model_repository
#           type: DirectoryOrCreate
#       # https://stackoverflow.com/questions/43373463/how-to-increase-shm-size-of-a-kubernetes-container-shm-size-equivalent-of-doc
#       - name: dshm
#         emptyDir:
#           medium: Memory
#           sizeLimit: 512Mi

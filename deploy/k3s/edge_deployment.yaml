apiVersion: v1
kind: Service
metadata:
  name: edge-endpoint
spec:
  selector:
    app: edge-endpoint
  ports:
  - protocol: TCP
    # Service port for NGINX
    port: 6717
    nodePort: 30101
  type: NodePort
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: edge-endpoint
  labels:
    app: edge-endpoint
spec:
  replicas: 1
  selector:
    matchLabels:
      app: edge-endpoint
  template:
    metadata:
      labels:
        app: edge-endpoint
    spec:
      containers:
      - name: edge-endpoint
        # Pull the image from ECR
        image: 723181461334.dkr.ecr.us-west-2.amazonaws.com/edge-endpoint:tyler-fetch-model-3165d7ae9-dirty-cc6c193f57b51de
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 6717
        env:
        - name: LOG_LEVEL
          value: "INFO"
        - name: GROUNDLIGHT_API_TOKEN
          valueFrom:
            secretKeyRef:
              name: groundlight-secrets
              key: api-token
        volumeMounts:
        - name: edge-config-volume
          # This is a path inside the container not the host
          mountPath: /etc/groundlight
        - name: model-repo
          mountPath: /mnt/models
      imagePullSecrets:
      - name: registry-credentials
      volumes:
      - name: edge-config-volume
        configMap:
          name: edge-config
      - name: model-repo
        hostPath:
          # A hostPath volume mounts a file or directory from the host node's filesystem into your Pod
          # This simiplifies things a lot, but only really works for single-node clusters.
          # TODO: check out k3s local path provisioner: https://docs.k3s.io/storage#setting-up-the-local-storage-provider
          path: /home/ubuntu/ptdev/zuuul2/predictors/serving/model_repository
          type: DirectoryOrCreate
---
apiVersion: v1
kind: Service
metadata:
  name: inference-service
spec:
  selector:
    app: inference-server
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inference-server
  labels:
    app: inference-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: inference-server
  template:
    metadata:
      labels:
        app: inference-server
    spec:
      containers:
      - name: inference-server
        image: 723181461334.dkr.ecr.us-west-2.amazonaws.com/gl-tritonserver:tyler-multiplatform-triton-build-06c8914c5-dirty-fe27f23cb32f186
        imagePullPolicy: IfNotPresent
        # Tritonserver will look for models in /mnt/models and initialize them on startup
        # TODO: We only want to shut down an old instance when this tritonsever gives the "READY" signal for all models
        command:
          ["tritonserver", "--model-repository=/mnt/models", "--load-model=*", "--model-control-mode=explicit"]
        resources:
          limits:
            nvidia.com/gpu: 1
        volumeMounts:
        - name: model-repo
          mountPath: /mnt/models
        - name: dshm
          mountPath: /dev/shm
        ports:
        - containerPort: 8000  # Default tritonserver HTTP port
      imagePullSecrets:
      - name: registry-credentials
      volumes:
      - name: model-repo
        hostPath:
          # A hostPath volume mounts a file or directory from the host node's filesystem into your Pod
          # This simiplifies things a lot, but only really works for single-node clusters.
          # TODO: check out k3s local path provisioner: https://docs.k3s.io/storage#setting-up-the-local-storage-provider
          path: /home/ubuntu/ptdev/zuuul2/predictors/serving/model_repository
          type: DirectoryOrCreate
      # https://stackoverflow.com/questions/43373463/how-to-increase-shm-size-of-a-kubernetes-container-shm-size-equivalent-of-doc
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 512Mi

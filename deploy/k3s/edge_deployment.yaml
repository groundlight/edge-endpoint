# Required runtime class for GPU use in k3s
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
---
apiVersion: v1
kind: Service
metadata:
  name: edge-endpoint
spec:
  selector:
    app: edge-endpoint
  ports:
  - protocol: TCP
    # Service port for NGINX
    port: 6717
    nodePort: 30101
  type: NodePort
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: edge-endpoint
  labels:
    app: edge-endpoint
spec:
  replicas: 1
  selector:
    matchLabels:
      app: edge-endpoint
  template:
    metadata:
      labels:
        app: edge-endpoint
    spec:
      containers:
      - name: edge-endpoint
        image: 723181461334.dkr.ecr.us-west-2.amazonaws.com/edge-endpoint:tyler-fix-patience-time-0d64ab0df-dirty-19cac12e4adca89
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 6717
        env:
        - name: LOG_LEVEL
          value: "DEBUG"
        - name: GROUNDLIGHT_API_TOKEN
          valueFrom:
            secretKeyRef:
              name: groundlight-secrets
              key: api-token
        volumeMounts:
        - name: edge-config-volume
          # This is a path inside the container not the host
          mountPath: /etc/groundlight
        - name: model-repo
          mountPath: /mnt/models
      imagePullSecrets:
      - name: registry-credentials
      volumes:
      - name: edge-config-volume
        configMap:
          name: edge-config
      - name: model-repo
        hostPath:
          # A hostPath volume mounts a file or directory from the host node's filesystem into your Pod
          # This simiplifies things a lot, but only really works for single-node clusters.
          # TODO: check out k3s local path provisioner: https://docs.k3s.io/storage#setting-up-the-local-storage-provider
          path: /home/ubuntu/ptdev/zuuul2/predictors/serving/model_repository
          type: DirectoryOrCreate
---
apiVersion: v1
kind: Service
metadata:
  name: inference-service
spec:
  selector:
    app: inference-server
  ports:
    - protocol: TCP
      port: 8000
      name: http
      targetPort: 8000
    - protocol: TCP
      port: 8002
      name: metrics  # From the edge machine, run curl localhost:<nodeport>/metrics to see metrics summary
      targetPort: 8002
  type: NodePort
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inference-server
  labels:
    app: inference-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: inference-server
  template:
    metadata:
      labels:
        app: inference-server
    spec:
      runtimeClassName: nvidia  # Required for GPU use in k3s
      containers:
      - name: inference-server
        image: 723181461334.dkr.ecr.us-west-2.amazonaws.com/gl-tritonserver:313680099-main
        imagePullPolicy: IfNotPresent
        # Tritonserver will look for models in /mnt/models and initialize them on startup
        # TODO: We only want to shut down an old instance when this tritonsever gives the "READY" signal for all models
        command:
          [
            "tritonserver",
            "--model-repository=/mnt/models",
            "--load-model=*",
            "--model-control-mode=explicit",
            "--metrics-config=summary_latencies=true"
          ]
        resources:
          limits:
            nvidia.com/gpu: 1
        volumeMounts:
        - name: model-repo
          mountPath: /mnt/models
        - name: dshm
          mountPath: /dev/shm
        ports:
        - containerPort: 8000
          name: http-triton
        - containerPort: 8002
          name: metrics-triton
      imagePullSecrets:
      - name: registry-credentials
      volumes:
      - name: model-repo
        hostPath:
          # A hostPath volume mounts a file or directory from the host node's filesystem into your Pod
          # This simiplifies things a lot, but only really works for single-node clusters.
          # TODO: check out k3s local path provisioner: https://docs.k3s.io/storage#setting-up-the-local-storage-provider
          path: /home/ubuntu/ptdev/zuuul2/predictors/serving/model_repository
          type: DirectoryOrCreate
      # https://stackoverflow.com/questions/43373463/how-to-increase-shm-size-of-a-kubernetes-container-shm-size-equivalent-of-doc
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 512Mi

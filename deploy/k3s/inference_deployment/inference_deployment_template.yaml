apiVersion: v1
kind: Service
metadata:
  name: placeholder-inference-service-name
spec:
  selector:
    app: inference-server
  ports:
    - protocol: TCP
      port: 8000
      name: http
      targetPort: 8000
    - protocol: TCP
      port: 8002
      name: metrics  # From the edge machine, run curl localhost:<nodeport>/metrics to see metrics summary
      targetPort: 8002
  type: NodePort
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: placeholder-inference-deployment-name
  labels:
    name: placeholder-inference-deployment-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: inference-server
  template:
    metadata:
      labels:
        app: inference-server
    spec:
      runtimeClassName: nvidia  # Required for GPU use in k3s
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxSurge: 1
          maxUnavailable: 0  # Aim for no downtime during rollout
      containers:
      - name: inference-server
        image: 767397850842.dkr.ecr.us-west-2.amazonaws.com/gl-tritonserver-test:latest
        imagePullPolicy: IfNotPresent
        # Tritonserver will look for models in /mnt/models and initialize them on startup.
        # When running multiple instances of Triton server on the same machine that use Python models,
        # there would be a conflict in the shared memory region name, which can lead to SEGFAULTs.
        # To avoid this, we can set the shm-region-prefix-name to a unique value for each Triton server instance.
        # TODO: We only want to shut down an old instance when this tritonsever gives the "READY" signal for all models
        # shm-default-byte-size=16777216
        command:
          [
            "tritonserver",
            "--model-repository=/opt/groundlight/edge/serving/model-repo",
            "--load-model=placeholder-model-name",
            "--allow-gpu-metrics=true",
            "--allow-cpu-metrics=true",
            "--model-control-mode=explicit",
            "--metrics-config=summary_latencies=true",
            "--backend-config=python,shm-region-prefix-name=placeholder-inference-deployment-name",
            "--log-verbose=1",
          ]
        volumeMounts:
        - name: edge-endpoint-persistent-volume
          mountPath: /opt/groundlight/edge/serving/model-repo

        - name: dshm
          mountPath: /dev/shm
        ports:
        - containerPort: 8000
          name: http-triton
        - containerPort: 8002
          name: metrics-triton
        readinessProbe:
          httpGet:
            path: /v2/health/ready  # Triton's readiness endpoint
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 30  # Wait for up to 5 min
      imagePullSecrets:
      - name: registry-credentials
      volumes:
      - name: edge-endpoint-persistent-volume
        persistentVolumeClaim:
          claimName: edge-endpoint-pvc
      # https://stackoverflow.com/questions/43373463/how-to-increase-shm-size-of-a-kubernetes-container-shm-size-equivalent-of-doc
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 128Mi
apiVersion: v1
kind: Service
metadata:
  name: placeholder-inference-service-name
spec:
  selector:
    app: inference-server
    instance: placeholder-inference-instance-name
  ports:
    - protocol: TCP
      port: 8000
      name: http
      targetPort: 8000
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: placeholder-inference-deployment-name
  labels:
    name: placeholder-inference-deployment-name
spec:
  replicas: 1
  selector:
    matchLabels:
      app: inference-server
      instance: placeholder-inference-instance-name
  template:
    metadata:
      labels:
        app: inference-server
        instance: placeholder-inference-instance-name
    spec:
      runtimeClassName: nvidia  # Required for GPU use in k3s
      # Redirect external model download requests to our MITM proxy
      hostAliases:
      - ip: "10.43.77.123"
        hostnames:
        - "huggingface.co"
        - "cdn-lfs.hf.co"
      imagePullSecrets:
      - name: registry-credentials
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxSurge: 1
          maxUnavailable: 0  # Aim for no downtime during rollout

      initContainers:
      - name: patch-certifi
        image: 767397850842.dkr.ecr.us-west-2.amazonaws.com/gl-edge-inference:latest
        imagePullPolicy: Always
        command: ["/bin/bash", "-c"]
        args:
        - |
          echo "Waiting for MITM proxy CA certificate..."
          TIMEOUT=30
          ELAPSED=0
          while [ ! -f /shared-cert/ca.crt ] && [ $ELAPSED -lt $TIMEOUT ]; do
            echo "CA cert not found, waiting... ($ELAPSED/$TIMEOUT seconds)"
            sleep 2
            ELAPSED=$((ELAPSED + 2))
          done
          
          if [ ! -f /shared-cert/ca.crt ]; then
            echo "Timeout waiting for CA certificate after $TIMEOUT seconds"
            exit 1
          fi
          
          echo "Found CA certificate, patching certifi bundle..."
          CERTIFI_PATH=$(python -c "import certifi; print(certifi.where())")
          echo "Certifi bundle location: $CERTIFI_PATH"
          
          # Backup original certifi bundle
          cp "$CERTIFI_PATH" "$CERTIFI_PATH.backup"
          
          # Append our CA certificate
          cat /shared-cert/ca.crt >> "$CERTIFI_PATH"
          
          echo "Successfully patched certifi bundle with MITM CA certificate"
          echo "Certifi bundle now contains $(wc -l < "$CERTIFI_PATH") lines"
        volumeMounts:
        - name: shared-cert-volume
          mountPath: /shared-cert
          readOnly: true

      # NOTE: the sync-pinamod container is duplicated in the warmup_inference_model.yaml Job
      # TODO: refactor to share code between the Job and the initContainer in the Deployment
      - name: sync-pinamod
        image: amazon/aws-cli:latest
        imagePullPolicy: IfNotPresent
        # Sync models and HF proxy content from S3 to the local hostmapped filesystem.
        command: ['sh', '-c']
        args:
        - |
          echo "Syncing pinamod models..."
          aws s3 sync s3://pinamod-artifacts-public/pinamod $PINAMOD_DIR --delete
          
          echo "Syncing HF proxy content..."
          aws s3 sync s3://pinamod-artifacts-public/hf-proxy $HF_CONTENT_DIR --delete
          
          echo "Sync complete"
        env:
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: aws-credentials
              key: aws_access_key_id
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: aws-credentials
              key: aws_secret_access_key
        - name: PINAMOD_DIR
          value: /opt/models/pinamod
        - name: HF_CONTENT_DIR
          value: /opt/hf-content
        volumeMounts:
        - name: pina-models
          mountPath: /opt/models
        - name: hf-content
          mountPath: /opt/hf-content

      containers:
      - name: inference-server
        image: 767397850842.dkr.ecr.us-west-2.amazonaws.com/gl-edge-inference:latest
        imagePullPolicy: Always
        env:
        - name: MODEL_REPOSITORY
          value: &modelRepository /opt/groundlight/edge/serving/model-repo
        - name: MODEL_NAME
          value: placeholder-model-name
        - name: PINAMOD_DIR
          value: /opt/models/pinamod
        - name: LOAD_ALL_PIPELINES  # Load only the pipelines that are needed for edge inference.
          value: "false"
        command:
          [
            "poetry", "run", "python3", "-m", "uvicorn", "serving.edge_inference_server.fastapi_server:app",
            "--host", "0.0.0.0",
            "--port", "8000",
            "--workers", "1"
          ]
        volumeMounts:
        - name: edge-endpoint-persistent-volume
          mountPath: *modelRepository
        - name: pina-models
          mountPath: /opt/models
          readOnly: true
        ports:
        - containerPort: 8000
          name: http-fastapi
        startupProbe:
          httpGet:
            path: /health/live  # Checks if the server is up
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 60  # Wait for up to 10 min
        readinessProbe:
          httpGet:
            path: /health/ready  # Checks if the server is ready to serve requests
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 10
          failureThreshold: 3  # after 30 seconds of failure, stop sending traffic to the pod
        livenessProbe:
          httpGet:
            # We use "ready" here because the current liveness probe is too simple and
            # doesn't check if the model is loaded.  If the models fail because of GPU memory
            # issues, we need this to fail and restart the pod.
            path: /health/ready
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 10
          failureThreshold: 6  # after 60 seconds of failure, restart the pod

      volumes:
      - name: edge-endpoint-persistent-volume
        persistentVolumeClaim:
          claimName: edge-endpoint-pvc
      - name: pina-models
        hostPath:
          path: /opt/groundlight/edge/pinamod-public
          type: Directory
      - name: shared-cert-volume
        hostPath:
          path: /opt/groundlight/ee-mitm-proxy/shared-cert
          type: DirectoryOrCreate
      - name: hf-content
        hostPath:
          path: /opt/groundlight/ee-mitm-proxy/content
          type: DirectoryOrCreate
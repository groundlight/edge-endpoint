# Required runtime class for GPU use in k3s
# Pods must have `runtimeClassName: nvidia` set to access GPU
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
---
apiVersion: v1
kind: Service
metadata:
  name: edge-endpoint-service
spec:
  selector:
    app: edge-logic-server
  ports:
    - protocol: TCP
      # Service port for NGINX
      port: 30101
      nodePort: ${EDGE_ENDPOINT_PORT}
  type: NodePort
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: edge-endpoint
  labels:
    app: edge-endpoint
spec:
  replicas: 1
  selector:
    matchLabels:
      app: edge-logic-server
  template:
    metadata:
      labels:
        app: edge-logic-server
    spec:
      # This service account is used by the edge logic to access the Kubernetes API
      # from within the pod. See deploy/k3s/service_account.yaml for more details
      serviceAccountName: edge-endpoint-service-account
      initContainers:
        - name: database-prep
          image: &edgeEndpointImage 767397850842.dkr.ecr.us-west-2.amazonaws.com/edge-endpoint:${IMAGE_TAG}
          imagePullPolicy: Always
          volumeMounts:
            - name: edge-endpoint-persistent-volume
              mountPath: /opt/groundlight/edge/sqlite
            - name: sqlite-db-setup-script-volume
              mountPath: /scripts
          command: ["/bin/bash", "/scripts/setup_db.sh"]

      containers:
        - name: edge-endpoint
          image: *edgeEndpointImage
          imagePullPolicy: Always
          ports:
            - containerPort: 30101
          env:
            - name: LOG_LEVEL
              value: "INFO"
            # We need this feature flag since we run the edge logic server in two separate environments:
            # 1. In docker (on the GitHub Actions runner) for testing
            # 2. In kubernetes (currently a dedicated EC2 instance)
            # This feature flag is basically good for knowing when to use the python kubernetes API
            # (i.e., creating deployments, etc.). We don't want to use the python kubernetes API
            # if we are only running the edge logic server in docker.
            # TODO: Once we have kubernetes-based tests, we can remove this feature flag.
            - name: DEPLOY_DETECTOR_LEVEL_INFERENCE
              value: "1"
          volumeMounts:
            - name: edge-config-volume
              mountPath: /etc/groundlight/edge-config
            - name: edge-endpoint-persistent-volume
              mountPath: /opt/groundlight/edge/sqlite
            - name: device-info-volume
              mountPath: /opt/groundlight/device
            - name: escalation-queue-volume
              mountPath: /opt/groundlight/queue
          startupProbe:
            httpGet:
              path: /health/live # Checks if the server is up
              port: 30101
            initialDelaySeconds: 10
            periodSeconds: 10
            failureThreshold: 6 # Wait for up to 1 min
          readinessProbe:
            httpGet:
              path: /health/ready # Checks if the server is ready to serve requests
              port: 30101
            initialDelaySeconds: 10
            periodSeconds: 10
            failureThreshold: 2 # after 20 seconds of failure, stop sending traffic to the pod
          livenessProbe:
            httpGet:
              path: /health/ready
              port: 30101
            initialDelaySeconds: 10
            periodSeconds: 10
            failureThreshold: 4 # after 40 seconds of failure, restart the pod

        - name: status-monitor
          image: *edgeEndpointImage
          imagePullPolicy: Always
          ports:
            - containerPort: 8123
          command: ["./app/bin/launch-status-monitor.sh"]
          env:
            - name: LOG_LEVEL
              value: "INFO"
            - name: GROUNDLIGHT_API_TOKEN
              valueFrom:
                secretKeyRef:
                  name: groundlight-api-token
                  key: GROUNDLIGHT_API_TOKEN
                  optional: true
            - name: DEPLOYMENT_NAMESPACE
              value: ${DEPLOYMENT_NAMESPACE}
          volumeMounts:
            - name: edge-config-volume
              mountPath: /etc/groundlight/edge-config
            - name: device-info-volume
              mountPath: /opt/groundlight/device

        - name: escalation-queue-reader
          image: *edgeEndpointImage
          imagePullPolicy: Always
          command: ["./app/bin/launch-escalation-queue-reader.sh"]
          env:
            - name: LOG_LEVEL
              value: "INFO"
            - name: GROUNDLIGHT_API_TOKEN
              valueFrom:
                secretKeyRef:
                  name: groundlight-api-token
                  key: GROUNDLIGHT_API_TOKEN
                  optional: true
          volumeMounts:
            - name: escalation-queue-volume
              mountPath: /opt/groundlight/queue

        - name: inference-model-updater
          image: *edgeEndpointImage
          imagePullPolicy: Always
          command: ["/bin/bash", "-c"]
          args: ["poetry run python -m app.model_updater.update_models"]
          env:
            - name: LOG_LEVEL
              value: "INFO"
            - name: DEPLOY_DETECTOR_LEVEL_INFERENCE
              value: "1"
            - name: GROUNDLIGHT_API_TOKEN
              valueFrom:
                secretKeyRef:
                  name: groundlight-api-token
                  key: GROUNDLIGHT_API_TOKEN
                  optional: true
          volumeMounts:
            - name: edge-config-volume
              mountPath: /etc/groundlight/edge-config

            # TODO: this should just be an environment variable
            - name: kubernetes-namespace
              mountPath: /etc/groundlight/kubernetes-namespace

            - name: inference-deployment-template-volume
              mountPath: /etc/groundlight/inference-deployment

            # In this setup the edge-endpoint-persistent-volume is mounted to
            # two different paths in the inference-model-updater container.
            # This allows the container to access both the sqlite database and
            # the path to the model repository without needing to create an extra PV and PVC.
            - name: edge-endpoint-persistent-volume
              mountPath: /opt/groundlight/edge/sqlite

            - name: edge-endpoint-persistent-volume
              mountPath: /opt/groundlight/edge/serving/model-repo

      imagePullSecrets:
        - name: registry-credentials

      volumes:
        - name: edge-config-volume
          configMap:
            name: edge-config
        - name: kubernetes-namespace
          configMap:
            name: kubernetes-namespace
        - name: inference-deployment-template-volume
          configMap:
            name: inference-deployment-template
        - name: sqlite-db-setup-script-volume
          configMap:
            name: setup-db
        - name: edge-endpoint-persistent-volume
          persistentVolumeClaim:
            claimName: edge-endpoint-pvc
        - name: device-info-volume
          hostPath:
            path: /opt/groundlight/device
            type: DirectoryOrCreate
        - name: escalation-queue-volume
          hostPath:
            path: /opt/groundlight/queue/${DEPLOYMENT_NAMESPACE}
            type: DirectoryOrCreate

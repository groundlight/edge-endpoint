apiVersion: v1
kind: Service
metadata:
  name: {{ INFERENCE_SERVICE_NAME }}
spec:
  selector:
    app: inference-server
  ports:
    - protocol: TCP
      port: 8000
      name: http
      targetPort: 8000
    - protocol: TCP
      port: 8002
      name: metrics  # From the edge machine, run curl localhost:<nodeport>/metrics to see metrics summary
      targetPort: 8002
  type: NodePort
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ DEPLOYMENT_NAME }}
  labels:
    app: {{ DEPLOYMENT_NAME }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: inference-server
  template:
    metadata:
      labels:
        app: inference-server
    spec:
      {{ - if .USE_GPU }}
      runtimeClassName: nvidia  # Required for GPU use in k3s
      {{ - end }}
      containers:
      - name: inference-server
        image: 723181461334.dkr.ecr.us-west-2.amazonaws.com/gl-tritonserver:313680099-main
        imagePullPolicy: IfNotPresent
        # Tritonserver will look for models in /mnt/models and initialize them on startup
        # When running multiple instances of Triton server on the same machine that use Python models, 
        # there would be a conflict in the shared memory region name, which can lead to SEGFAULTs. 
        # To avoid this, we can set the shm-region-prefix-name to a unique value for each Triton server instance.
        # TODO: We only want to shut down an old instance when this tritonsever gives the "READY" signal for all models
        command:
          [
            "tritonserver",
            "--model-repository=/mnt/models",
            "--load-model=*",
            "--model-control-mode=explicit",
            "--metrics-config=summary_latencies=true"
            "--backend-config=python,shm-region-prefix-name={{ DEPLOYMENT_NAME }}"
          ]
        resources:
          {{ - if .USE_GPU }}
          limits:
            nvidia.com/gpu: 1
          {{ - end }}
        volumeMounts:
        - name: model-repo
          mountPath: /mnt/models
        - name: dshm
          mountPath: /dev/shm
        ports:
        - containerPort: 8000
          name: http-triton
        - containerPort: 8002
          name: metrics-triton
      imagePullSecrets:
      - name: registry-credentials
      volumes:
      - name: model-repo
        hostPath:
          # A hostPath volume mounts a file or directory from the host node's filesystem into your Pod
          # This simiplifies things a lot, but only really works for single-node clusters.
          # TODO: check out k3s local path provisioner: https://docs.k3s.io/storage#setting-up-the-local-storage-provider
          path: /home/ubuntu/ptdev/zuuul2/predictors/serving/model_repository
          type: DirectoryOrCreate
      # https://stackoverflow.com/questions/43373463/how-to-increase-shm-size-of-a-kubernetes-container-shm-size-equivalent-of-doc
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 512Mi
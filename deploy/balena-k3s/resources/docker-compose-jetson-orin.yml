version: "2.4"

# Balena docker-compose.yml for standing up a k3s server (with NVIDIA GPU support) and bastion
# Limitations: Currently only works on `Jetson Nano Orin` devices.

services:
  server-jetson:
    # https://docs.k3s.io/advanced#running-k3s-in-docker
    build:
      context: deploy/balena-k3s/server-jetson-orin
      args:
        # This args does not do anything since Jetpack comes with the NVIDIA drivers pre-installed and it is not possible to install a different version. 540.3.0 is the version that comes with L4T 36.3 (Jetpack 6.0), CUDA version 12.2.
        NVIDIA_DRIVER_VERSION: "540.3.0"
        # Setting ENABLE_GPU to true causes nvidia drivers, nvidia container toolkit, and nvidia-gpu-operator to be installed.
        # But it does not mandate that the server will use the GPU. The server will use the GPU only if the INFERENCE_FLAVOR is set to "GPU".
        ENABLE_GPU: &enable_gpu true
    ulimits:
      nproc: 65535
      nofile:
        soft: 65535
        hard: 65535
    privileged: true
    network_mode: host
    environment:
      # https://docs.k3s.io/cli/server#cluster-options
      K3S_KUBECONFIG_OUTPUT: "/shared/kubeconfig.yaml"
      K3S_KUBECONFIG_MODE: "644"
      EXTRA_K3S_SERVER_ARGS: "--disable traefik"
      # Set this to "GPU" to enable GPU support. This simply makes the GPU visible to the inference container.
      INFERENCE_FLAVOR: "GPU"
      ENABLE_GPU: *enable_gpu
    tmpfs:
      - /run
      - /var/run
    volumes:
      - k3s-server:/var/lib/rancher/k3s
      - shared:/shared

  bastion:
    build:
      context: .
      dockerfile: deploy/balena-k3s/bastion/Dockerfile
    network_mode: host
    environment:
      KUBECONFIG: "/shared/kubeconfig.yaml"
    volumes:
      - shared:/shared:ro
    depends_on:
      - server-jetson

volumes:
  k3s-server: {}
  shared: {}
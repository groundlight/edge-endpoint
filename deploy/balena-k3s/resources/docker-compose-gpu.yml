version: "2.4"

# Balena docker-compose.yml for standing up a k3s server (with NVIDIA GPU support) and bastion
# Limitations: Currently only works on `generic-amd64` devices.

services:
  gpu:
    build:
      context: deploy/balena-k3s/gpu
      args:
        NVIDIA_DRIVER_VERSION: &nvidia-driver "570.133.07"
    privileged: true

  server:
    # https://docs.k3s.io/advanced#running-k3s-in-docker
    build:
      context: deploy/balena-k3s/server
      args:
        NVIDIA_DRIVER_VERSION: *nvidia-driver
        # Setting ENABLE_GPU to true causes nvidia drivers, nvidia container toolkit, and nvidia-gpu-operator to be installed.
        # But it does not mandate that the server will use the GPU. The server will use the GPU only if the INFERENCE_FLAVOR is set to "GPU".
        ENABLE_GPU: &enable_gpu true
    ulimits:
      nproc: 65535
      nofile:
        soft: 65535
        hard: 65535
    privileged: true
    network_mode: host
    environment:
      # https://docs.k3s.io/cli/server#cluster-options
      K3S_KUBECONFIG_OUTPUT: "/shared/kubeconfig.yaml"
      K3S_KUBECONFIG_MODE: "644"
      EXTRA_K3S_SERVER_ARGS: "--disable traefik"
      # Set this to "GPU" to enable GPU support. This simply makes the GPU visible to the inference container.
      INFERENCE_FLAVOR: "GPU"
      ENABLE_GPU: *enable_gpu
    tmpfs:
      - /run:rshared,exec
      - /var/run:rshared,exec
    volumes:
      - k3s-server:/var/lib/rancher/k3s
      - shared:/shared
      - pinamod:/opt/groundlight/edge/pinamod-public
    depends_on:
      - gpu

  bastion:
    build:
      context: .
      dockerfile: deploy/balena-k3s/bastion/Dockerfile
    network_mode: host
    environment:
      KUBECONFIG: "/shared/kubeconfig.yaml"
    volumes:
      - shared:/shared:ro
    depends_on:
      - server

volumes:
  k3s-server: {}
  shared: {}
  pinamod: {}
